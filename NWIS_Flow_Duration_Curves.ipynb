{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f01a0b",
   "metadata": {},
   "source": [
    "# NWIS Flow Duration Curves\n",
    "\n",
    "This notebook demonstrates how retrieve data from NWIS and use it to estimate flow duration curves. It borrows heavily from the work done by Paul Inkenbrandt and published here: http://earthpy.org/flow.htmlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import ulmo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp\n",
    "import scipy.optimize as op\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fb660",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The function `download_usgs_site_data` uses the <a href=http://ulmo.readthedocs.org/en/latest/>`ulmo`</a> package to retrieve U.S. Geological Survey surface water site data from the <a href=http://waterdata.usgs.gov/nwis>National Water Information System (NWIS) website</a>.  The function also puts the data from the website into a usable format. `download_usgs_site_info` gets site information from NWIS, which includes site name, watershed size, and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_usgs_site_data(siteno):\n",
    "    # Download data\n",
    "    data = ulmo.usgs.nwis.get_site_data(siteno, service=\"daily\", period=\"all\")\n",
    "    \n",
    "    # Parse values into dataframe\n",
    "    values = pd.DataFrame(data['00060:00003']['values'])\n",
    "    values['dates'] = pd.to_datetime(pd.Series(values['datetime']))\n",
    "    values.set_index(['dates'], inplace=True)\n",
    "    values[siteno] = values['value'].astype(float)\n",
    "    values[str(siteno)+'qual'] = values['qualifiers']\n",
    "    values = values.drop(['datetime','qualifiers','value'], axis=1)\n",
    "    values = values.replace('-999999', np.NAN)\n",
    "    values = values.dropna()\n",
    "    \n",
    "    # Parse site info into dataframe\n",
    "    info = pd.DataFrame(data['00060:00003']['site'])\n",
    "    info['latitude'] = info.loc['latitude', 'location']\n",
    "    info['longitude'] = info.loc['longitude', 'location']\n",
    "    info['latitude'] = info['latitude'].astype(float)\n",
    "    info['longitude'] = info['longitude'].astype(float)\n",
    "    info = info.drop(['default_tz', 'dst_tz', 'srs', 'uses_dst', 'longitude'], axis=0)\n",
    "    info = info.drop(['agency', 'timezone_info', 'location', 'state_code', 'network'], axis=1)\n",
    "    return info, values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf11975",
   "metadata": {},
   "source": [
    "`fdc` generates a flow duration curve for hydrologic data. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a797c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdc(df, site, begyear=1900, endyear=2022, normalizer=1):\n",
    "    '''\n",
    "    Generate flow duration curve for hydrologic time series data\n",
    "    \n",
    "    PARAMETERS:\n",
    "        df = pandas dataframe of interest; must have a date or date-time as the index\n",
    "        site = pandas column containing discharge data; must be within df\n",
    "        begyear = beginning year of analysis; defaults to 1900\n",
    "        endyear = end year of analysis; defaults to 2022\n",
    "        normalizer = value to use to normalize discharge; defaults to 1 (no normalization)\n",
    "    \n",
    "    RETURNS:\n",
    "        probability, discharge\n",
    "        \n",
    "    REQUIRES:\n",
    "        numpy as np\n",
    "        pandas as pd\n",
    "        matplotlib.pyplot as plt\n",
    "        scipy.stats as sp\n",
    "    '''\n",
    "    # limit dataframe to only the site\n",
    "    df = df[[site]]\n",
    "    \n",
    "    # filter dataframe to only include dates of interest\n",
    "    dt_index = pd.to_datetime(df.index)\n",
    "    data = df[(dt_index > dt.datetime(begyear, 1, 1))&(dt_index < dt.datetime(endyear, 1, 1))]\n",
    "\n",
    "    # remove na values from dataframe\n",
    "    data = data.dropna()\n",
    "\n",
    "    # take average of each day of year (from 1 to 366) over the selected period of record\n",
    "    data['doy'] = data.index.dayofyear\n",
    "    dailyavg = data[site].groupby(data['doy']).mean()\n",
    "        \n",
    "    data = np.sort(dailyavg)\n",
    "\n",
    "    ## uncomment the following to use normalized discharge instead of discharge\n",
    "    #mean = np.mean(data)\n",
    "    #std = np.std(data)\n",
    "    #data = [(data[i]-np.mean(data))/np.std(data) for i in range(len(data))]\n",
    "    data = [(data[i])/normalizer for i in range(len(data))]\n",
    "    \n",
    "    # ranks data from smallest to largest\n",
    "    ranks = sp.rankdata(data, method='average')\n",
    "\n",
    "    # reverses rank order\n",
    "    ranks = ranks[::-1]\n",
    "    \n",
    "    # calculate probability of each rank\n",
    "    prob = [(ranks[i]/(len(data)+1)) for i in range(len(data)) ]\n",
    "    \n",
    "    return prob, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1511c",
   "metadata": {},
   "source": [
    "The `fdcmatch` uses Python's optimization capabilities to fit natural logarithim and exponential functions the flow duration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdcmatch(df, site, begyear=1900, endyear=2015, normalizer=1, fun=1):\n",
    "    '''\n",
    "    * This function creates a flow duration curve (or its inverse) and then matches a natural logrithmic function (or its inverse - exp) \n",
    "    to the flow duration curve\n",
    "    * The flow duration curve will be framed for averaged daily data for the duration of one year (366 days)\n",
    "    \n",
    "    PARAMETERS:\n",
    "        df = pandas dataframe of interest; must have a date or date-time as the index\n",
    "        site = pandas column containing discharge data; must be within df\n",
    "        begyear = beginning year of analysis; defaults to 1900\n",
    "        endyear = end year of analysis; defaults to 2015\n",
    "        normalizer = value to use to normalize discharge; defaults to 1 (no normalization)\n",
    "        fun = 1 for probability as a function of discharge; 0 for discharge as a function of probability; default=1 \n",
    "            * 1 will choose:\n",
    "                prob = a*ln(discharge*b+c)+d\n",
    "            * 0 will choose:\n",
    "                discharge = a*exp(prob*b+c)+d\n",
    "    RETURNS:\n",
    "        para, parb, parc, pard, r_squared_value, stderr\n",
    "    \n",
    "        par = modifying variables for functions = a,b,c,d\n",
    "        r_squared_value = r squared value for model\n",
    "        stderr = standard error of the estimate\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "        pandas, scipy, numpy\n",
    "    '''\n",
    "    df = df[[site]]\n",
    "    \n",
    "    # filter dataframe to only include dates of interest\n",
    "    dt_index = pd.to_datetime(df.index)\n",
    "    data = df[(dt_index > dt.datetime(begyear, 1, 1))&(dt_index < dt.datetime(endyear, 1, 1))]\n",
    "\n",
    "    # remove na values from dataframe\n",
    "    data = data.dropna()\n",
    "\n",
    "    # take average of each day of year (from 1 to 366) over the selected period of record\n",
    "    data['doy']=data.index.dayofyear\n",
    "    dailyavg = data[site].groupby(data['doy']).mean()\n",
    "        \n",
    "    data = np.sort(dailyavg)\n",
    "\n",
    "    ## uncomment the following to use normalized discharge instead of discharge\n",
    "    #mean = np.mean(data)\n",
    "    #std = np.std(data)\n",
    "    #data = [(data[i]-np.mean(data))/np.std(data) for i in range(len(data))]\n",
    "    data = [(data[i])/normalizer for i in range(len(data))]\n",
    "    \n",
    "    # ranks data from smallest to largest\n",
    "    ranks = sp.rankdata(data, method='average')\n",
    "\n",
    "    # reverses rank order\n",
    "    ranks = ranks[::-1]\n",
    "    \n",
    "    # calculate probability of each rank\n",
    "    prob = [(ranks[i]/(len(data)+1)) for i in range(len(data)) ]\n",
    " \n",
    "    # choose which function to use\n",
    "    try:\n",
    "        if fun == 1:\n",
    "            # function to determine probability as a function of discharge\n",
    "            def func(x, a, b, c, d):\n",
    "                return a*np.log(x*b+c)+d\n",
    "\n",
    "            # matches func to data\n",
    "            par, cov = op.curve_fit(func, data, prob)\n",
    "\n",
    "            # checks fit of curve match\n",
    "            slope, interecept, r_value, p_value, stderr = \\\n",
    "            sp.linregress(prob, [par[0]*np.log(data[i]*par[1]+par[2])+par[3] for i in range(len(data))])\n",
    "        else:\n",
    "            # function to determine discharge as a function of probability\n",
    "            def func(x, a, b, c, d):\n",
    "                return a*np.exp(x*b+c)+d\n",
    "\n",
    "            # matches func to data\n",
    "            par, cov = op.curve_fit(func, prob, data)\n",
    "\n",
    "            # checks fit of curve match\n",
    "            slope, interecept, r_value, p_value, stderr = \\\n",
    "            sp.linregress(data, [par[0]*np.exp(prob[i]*par[1]+par[2])+par[3] for i in range(len(prob))])\n",
    "\n",
    "        # return parameters (a,b,c,d), r-squared of model fit, and standard error of model fit \n",
    "        return par[0], par[1], par[2], par[3], round(r_value**2,2), round(stderr,5)\n",
    "    except (RuntimeError, TypeError, ValueError):\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099a407",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "The list `sites` designates the USGS surface sites you are interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['09309600','09309800', '09310000','09310500','09310700','09311500','09312700','09313000',\n",
    "         '09317000','09317919','09317920','09317997','09318000','09318500','09319000',\n",
    "         '09323000','09324000','09324200','09324500','09325000','09325100','09326500','09327500','09327550',\n",
    "         '09330500','09331900','09331950','09332100', '10148500',\n",
    "         '10205030','10206000','10206001','10208500',\n",
    "         '10210000','10211000','10215700','10215900','10216210','10216400','10217000']\n",
    "sitelab = ['Q' + site for site in sites]\n",
    "print(len(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b9a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = dict()\n",
    "z = dict()\n",
    "for site in sites:\n",
    "    z[site], d[site] = download_usgs_site_data(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b1ab2",
   "metadata": {},
   "source": [
    "Merge data into a single dataframe so the data are aligned. Also, add day, month, and year columns to make summarizing the data more simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7607d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = dict()\n",
    "f[sites[0]] = d[sites[0]]\n",
    "for i in range(len(sites) - 1):\n",
    "    f[sites[i + 1]] = pd.merge(d[sites[i + 1]], f[sites[i]], left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_Site_Data = f[sites[-1]]\n",
    "USGS_Site_Data['mon'] = USGS_Site_Data.index.month\n",
    "USGS_Site_Data['yr'] = USGS_Site_Data.index.year\n",
    "USGS_Site_Data['dy'] = USGS_Site_Data.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4714c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_Site_Info = pd.concat(z)\n",
    "USGS_Site_Info = USGS_Site_Info.reset_index()\n",
    "USGS_Site_Info = USGS_Site_Info.drop(['level_1'],axis=1)\n",
    "USGS_Site_Info = USGS_Site_Info.set_index(['level_0'])\n",
    "USGS_Site_Info = USGS_Site_Info.drop(['code'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affbc33",
   "metadata": {},
   "source": [
    "Lets extract the measurement start and end dates from the station data, as well as some basic summary statistics. We can tack this information onto the site information table we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dict()\n",
    "m = dict()\n",
    "for site in sites:\n",
    "    q[site] = USGS_Site_Data[site].first_valid_index()\n",
    "    m[site] = USGS_Site_Data[site].last_valid_index()\n",
    "\n",
    "USGS_start_date = pd.DataFrame(data=q, index=[0])\n",
    "USGS_finish_date = pd.DataFrame(data=m, index=[0])\n",
    "USGS_start_date = USGS_start_date.transpose()\n",
    "USGS_start_date['start_date'] = USGS_start_date[0]\n",
    "USGS_start_date = USGS_start_date.drop([0],axis=1)\n",
    "USGS_finish_date = USGS_finish_date.transpose()\n",
    "USGS_finish_date['fin_date'] = USGS_finish_date[0]\n",
    "USGS_finish_date = USGS_finish_date.drop([0],axis=1)\n",
    "USGS_start_fin = pd.merge(USGS_finish_date,USGS_start_date, left_index=True, right_index=True, how='outer')\n",
    "USGS_Site_Info = pd.merge(USGS_start_fin,USGS_Site_Info, left_index=True, right_index=True, how='outer')\n",
    "USGS_sum_stats = USGS_Site_Data[sites].describe()\n",
    "USGS_sum_stats = USGS_sum_stats.transpose()\n",
    "USGS_Site_Info = pd.merge(USGS_sum_stats,USGS_Site_Info, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800cb1a",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "\n",
    "Save the data downloaded to CSV so it doesn't need to be downloaded again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e133204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/tethys/apps/awra_demo_2022/hydrology/\"\n",
    "USGS_Site_Data.to_csv(data_root + 'USGS_Site_Data.csv')\n",
    "USGS_Site_Info.to_csv(data_root + 'USGS_Site_Info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16834c",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Use this to load the data from CSV in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e57767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to load data from previously saved file\n",
    "# data_root = \"/home/tethys/apps/awra_demo_2022/hydrology/\"\n",
    "# USGS_Site_Data = pd.read_csv(data_root + 'USGS_Site_Data.csv', low_memory=False)\n",
    "# USGS_Site_Info = pd.read_csv(data_root + 'USGS_Site_Info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9d466",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The next two lines will display a summary of what is contained in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ddd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_Site_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb63843",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_Site_Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c5b3e",
   "metadata": {},
   "source": [
    "### Map Sites\n",
    "\n",
    "Use Plotly's Scattermapbox module to render a simple map of the sites. Convert the DataFrame to a GeoDataFrame from geopandas for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ec2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Scattermapbox(\n",
    "    mode=\"markers\",\n",
    "    lon=USGS_Site_Info.longitude,\n",
    "    lat=USGS_Site_Info.latitude,\n",
    "    marker={'size': 10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'center': {'lon': -111, 'lat': 38.7},\n",
    "        'style': \"stamen-terrain\",\n",
    "        'zoom': 6\n",
    "})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1bd5e",
   "metadata": {},
   "source": [
    "### Flow Duration Curves\n",
    "\n",
    "This example produces the Flow Duration Curves for the given station. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e407d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_index = 2\n",
    "site_no = USGS_Site_Info.index[site_index]\n",
    "site_name = USGS_Site_Info['name'][site_index]\n",
    "site_beg_record = str(USGS_Site_Info['start_date'][site_index])[0:10]\n",
    "site_fin_record = str(USGS_Site_Info['fin_date'][site_index])[0:10]\n",
    "\n",
    "# Calculate the flow duration curves\n",
    "prob1, discharge1 = fdc(USGS_Site_Data, site_no, 1900, 2020)\n",
    "prob2, discharge2 = fdc(USGS_Site_Data, site_no, 1900, 1970)\n",
    "prob3, discharge3 = fdc(USGS_Site_Data, site_no, 1970, 2020)\n",
    "\n",
    "# Plot with plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=prob1, y=discharge1, name=f'{site_no} 1900-2020'))\n",
    "fig.add_trace(go.Scatter(x=prob2, y=discharge2, name=f'{site_no} 1900-1970'))\n",
    "fig.add_trace(go.Scatter(x=prob3, y=discharge3, name=f'{site_no} 1970-2020'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Flow duration curve for {site_name} ({site_no})<br>\"\n",
    "          f\"Record: {site_beg_record} to {site_fin_record}\",\n",
    "    xaxis_title='probability that discharge was exceeded or equaled',\n",
    "    xaxis_dtick=0.05,\n",
    "    yaxis_title='discharge (cfs)',\n",
    "    yaxis_type='log'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ed178",
   "metadata": {},
   "source": [
    "## Write to GeoJSON for Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_Site_Info = gpd.GeoDataFrame(\n",
    "    USGS_Site_Info, \n",
    "    geometry=gpd.points_from_xy(USGS_Site_Info.longitude, USGS_Site_Info.latitude)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b821c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def serializer(o):\n",
    "    return str(o)\n",
    "    \n",
    "\n",
    "USGS_Site_Info.to_json(default=serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('USGS_Site_Info.geojson', 'w') as f:\n",
    "    f.write(USGS_Site_Info.to_json(default=serializer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
